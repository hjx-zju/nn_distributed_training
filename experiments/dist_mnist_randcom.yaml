experiment:
  name: "MNIST_RANDCOM"
  data_dir: "../data/"
  output_metadir: "../results/"
  use_cuda: true
  writeout: true
  data_split_type: "hetero"
  loss: "NLL"
  graph:
    num_nodes: 10
    type: "cycle"
    p: 0.3
    gen_attempts: 100
  model:
    num_filters: 3
    kernel_size: 5
    linear_width: 64
  individual_training:
    train_solo: False
    optimizer: "adam"
    lr: 0.005
    epochs: 6
    train_batch_size: 100
    val_batch_size: 100
    verbose: true
problem_configs:
  problem1:
    problem_name: "randcom"
    train_batch_size: 64
    val_batch_size: 128
    verbose_evals: true
    metrics:
      - "forward_pass_count"
      - "validation_loss"
      - "consensus_error"
      - "top1_accuracy"
      - "current_epoch"
    metrics_config:
      evaluate_frequency: 20
    optimizer_config:
      alg_name: "randcom"
      outer_iterations: 2800
      alpha: 0.01
      p: 0.7
      beta: 1
      gamma: 1
      init_grads: False
      profile: false
